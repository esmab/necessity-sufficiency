{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fb9d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from random import shuffle\n",
    "from perturbation_functions import get_preds_and_scores, calc_suff, calc_necc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e44157",
   "metadata": {},
   "outputs": [],
   "source": [
    "perts = pickle.load(open(\"Data/HateCheck_necc_suff_perturbations.pickle\",\"rb\"))\n",
    "perts['orig_texts'] = [tt.strip(' \\n') for tt in perts['orig_texts']]\n",
    "perts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b9c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# add special tokens for URLs, emojis and mentions (--> see pre-processing)\n",
    "special_tokens_dict = {'additional_special_tokens': ['[USER]','[EMOJI]','[URL]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "datasets = ['CAD_abuse', \n",
    "            'Davidson_abuse', \n",
    "            'Founta_abuse',\n",
    "            'CAD_hate',\n",
    "            'Davidson_hate',\n",
    "            'Founta_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a000c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "orig_preds = {}\n",
    "orig_scores = {}\n",
    "necc_preds = {}\n",
    "necc_scores = {}\n",
    "suff_preds = {}\n",
    "suff_scores = {}\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Classifying HateCheck perturbations with {}.\".format(dataset))\n",
    "    model = BertForSequenceClassification.from_pretrained(\"Models/Classifiers/{}\".format(dataset))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()\n",
    "    \n",
    "    total_len = len(perts['orig_texts']) + sum(len(nn) for nn in perts['necc_perturbed']) + sum(len(nn) for nn in perts['suff_perturbed'])\n",
    " \n",
    "    with tqdm(total=total_len) as pbar:\n",
    "        orig_preds[dataset], orig_scores[dataset] = get_preds_and_scores(perts['orig_texts'], tokenizer, model, pbar)\n",
    "        \n",
    "        necc_preds[dataset] = []\n",
    "        necc_scores[dataset] = []\n",
    "    \n",
    "        for tt in perts['necc_perturbed']:\n",
    "            pp, ss = get_preds_and_scores(tt, tokenizer, model, pbar)\n",
    "            necc_preds[dataset].append(pp)\n",
    "            necc_scores[dataset].append(ss)\n",
    "            \n",
    "        suff_preds[dataset] = []\n",
    "        suff_scores[dataset] = []\n",
    "    \n",
    "        for tt in perts['suff_perturbed']:\n",
    "            pp, ss = get_preds_and_scores(tt, tokenizer, model, pbar)\n",
    "            suff_preds[dataset].append(pp)\n",
    "            suff_scores[dataset].append(ss)\n",
    "            \n",
    "        \n",
    "final_results = {\n",
    "                'orig_preds': orig_preds,\n",
    "                'orig_scores': orig_scores,\n",
    "                'necc_preds': necc_preds,\n",
    "                'necc_scores': necc_scores,\n",
    "                'suff_preds': suff_preds,\n",
    "                'suff_scores': suff_scores,\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4837fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_results, open(\"Data/HateCheck_necc_suff_preds.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36b5706",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/ILM/compound_dataset/train.txt\", \"r\") as ff:\n",
    "    compound_dataset = ff.read().split(\"\\n\\n\\n\")\n",
    "compound_dataset = [tt.strip(\" :`.,\") for tt in compound_dataset]\n",
    "shuffle(compound_dataset)\n",
    "compound_dataset = compound_dataset[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e0a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_preds = {}\n",
    "baseline_scores = {}\n",
    "for dataset in datasets: \n",
    "    model = BertForSequenceClassification.from_pretrained(\"Models/Classifiers/{}\".format(dataset))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()\n",
    "    preds, scores = get_preds_and_scores(compound_dataset, tokenizer, model)\n",
    "    baseline_preds[dataset] = sum(preds)/len(preds)\n",
    "    baseline_scores[dataset] = sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e9ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump({'baseline_preds':baseline_preds, 'baseline_scores':baseline_scores}, open(\"Classifier_baselines.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be4b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "necc_results = {}\n",
    "necc_results_nb = {}\n",
    "suff_results = {}\n",
    "suff_results_nb = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    ## NECCESSITY CALCULATIONS\n",
    "    neccs = []\n",
    "    for oo, pp, mm in zip(final_results['orig_preds'][dataset], \n",
    "                          final_results['necc_preds'][dataset], \n",
    "                          perts['necc_masks']):\n",
    "        pp = np.array(pp)\n",
    "        neccs.append(calc_necc(oo, pp, mm))\n",
    "    necc_results[dataset] = neccs \n",
    "    \n",
    "    neccs_nb = []\n",
    "    for oo, pp, mm in zip(final_results['orig_scores'][dataset], \n",
    "                          final_results['necc_scores'][dataset], \n",
    "                          perts['necc_masks']):\n",
    "        pp = np.array(pp)\n",
    "        neccs_nb.append(calc_necc(oo, pp, mm))\n",
    "    necc_results_nb[dataset] = neccs_nb\n",
    "    \n",
    "    ## SUFFICIENCY CALCULATIONS\n",
    "    baseline_pred = baseline_preds[dataset]\n",
    "    baseline_score = baseline_scores[dataset]\n",
    "    \n",
    "    suffs = []\n",
    "    for pp, mm in zip(final_results['suff_preds'][dataset], perts['suff_masks']):\n",
    "        pp = np.array(pp)\n",
    "        suffs.append(calc_suff(baseline_pred, pp, mm))\n",
    "    suff_results[dataset] = suffs \n",
    "    \n",
    "    suffs_nb = []\n",
    "    for pp, mm in zip(final_results['suff_scores'][dataset], perts['suff_masks']):\n",
    "        pp = np.array(pp)\n",
    "        suffs_nb.append(calc_suff(baseline_score, pp, mm))\n",
    "    suff_results_nb[dataset] = suffs_nb     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23175599",
   "metadata": {},
   "outputs": [],
   "source": [
    "hatecheck_necc_suff_results = {\n",
    "    'necc_results': necc_results,\n",
    "    'necc_results_nb': necc_results_nb,\n",
    "    'suff_results': suff_results, \n",
    "    'suff_results_nb': suff_results_nb\n",
    "}\n",
    "\n",
    "pickle.dump(hatecheck_necc_suff_results, open('Data/HateCheck_necc_suff_results_all.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a850153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the predictions for all models for the entire hatecheck suite\n",
    "hc_test_cases_all = pd.read_csv(\"hatecheck-data/test_suite_cases.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577d8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_test_cases_all = hc_test_cases_all.test_case.tolist()\n",
    "hc_preds = {}\n",
    "hc_scores = {}\n",
    "for dataset in datasets: \n",
    "    model = BertForSequenceClassification.from_pretrained(\"Models/Classifiers/{}\".format(dataset))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()\n",
    "    preds, scores = get_preds_and_scores(hc_test_cases_all, tokenizer, model)\n",
    "    hc_preds[dataset] = preds\n",
    "    hc_scores[dataset] = scores\n",
    "\n",
    "pickle.dump({'preds': hc_preds, 'scores':hc_scores}, open('Data/HateCheck_results_all_models.pickle', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19e934",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    hc_test_cases_all['{}_pred'.format(dataset)] = hc_preds[dataset]\n",
    "    hc_test_cases_all['{}_score'.format(dataset)] = hc_scores[dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31924981",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(hc_test_cases_all, open('Data/HateCheck_templates_and_results.pickle', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b357da42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
